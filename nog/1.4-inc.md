# Mixed Precision Training: FP16/BF16 Memory Layouts and Gradient Scaling

**Part 4 of "Distributed Training from Scratch"**

In 1985, the IEEE 754 standard declared that 32 bits should be enough for anyone to represent a floating point number. For the next three decades, we dutifully stored every weight, every gradient, every activation in glorious FP32 precision, as if the universe itself demanded that level of numerical fidelity. Then someone at NVIDIA did the math and realized we were basically using a Ferrari to deliver pizza. Welcome to mixed precision training, where we've discovered that most of our neural networks are surprisingly tolerant of numerical approximation, and cutting our memory usage in half is worth the occasional gradient that vanishes into the numerical void.

The beautiful irony is that mixed precision training represents a kind of computational maturity we've been slowly approaching for decades. Just as Turing showed us that infinite computational power could emerge from the simplest operations, mixed precision shows us that acceptable intelligence can emerge from deliberately imprecise computation. We're trading numerical purity for practical scale, and it turns out the neural networks don't actually care as much as we thought they would.

This is the post where we learn to love approximation, embrace numerical instability, and somehow make our models train faster while using half the memory. It's like discovering you can run a marathon in flip-flops – counterintuitive, slightly dangerous, but surprisingly effective if you know what you're doing.

## The Precision Paradox: When Less is More

Let's start with a reality check about what we're actually doing when we train neural networks. We're essentially performing millions of tiny numerical updates, hoping that the accumulated effect of all these micro-adjustments will somehow converge to something intelligent. The question is: do we really need 32 bits of precision for each of these adjustments?

The answer, as it turns out, is a resounding "mostly no." Neural networks exhibit a remarkable property called **precision resilience** – they can tolerate significant numerical approximation without losing their ability to learn useful representations. This isn't just convenient; it's almost philosophically profound. The intelligence that emerges from these systems doesn't require perfect numerical precision. It's robust to the kind of approximation that makes engineers uncomfortable but makes GPUs very, very happy.

```python
import torch
import torch.nn as nn
from torch.cuda.amp import autocast, GradScaler
import time

class PrecisionComparison:
    """Compare FP32 vs Mixed Precision training"""
    
    def __init__(self, model_size=1000000):
        self.model_fp32 = self.create_test_model(model_size)
        self.model_fp16 = self.create_test_model(model_size)
        
        # Mixed precision requires a gradient scaler
        self.scaler = GradScaler()
        
    def create_test_model(self, size):
        """Create a ridiculously large linear layer for testing"""
        return nn.Sequential(
            nn.Linear(size, size),
            nn.ReLU(),
            nn.Linear(size, 1000),
            nn.LogSoftmax(dim=-1)
        ).cuda()
        
    def memory_usage_comparison(self):
        """Compare memory usage between precisions"""
        
        # FP32 model
        fp32_params = sum(p.numel() * 4 for p in self.model_fp32.parameters())  # 4 bytes per param
        
        # FP16 would use half
        fp16_params = sum(p.numel() * 2 for p in self.model_fp16.parameters())  # 2 bytes per param
        
        print(f"FP32 model: {fp32_params / 1e9:.2f} GB")
        print(f"FP16 model: {fp16_params / 1e9:.2f} GB")
        print(f"Memory savings: {(1 - fp16_params/fp32_params) * 100:.1f}%")
        
        # But wait, there's more! Activations also benefit
        batch_size, seq_len, hidden_dim = 32, 512, 4096
        activation_fp32 = batch_size * seq_len * hidden_dim * 4  # bytes
        activation_fp16 = batch_size * seq_len * hidden_dim * 2  # bytes
        
        print(f"\nActivation memory per layer:")
        print(f"FP32: {activation_fp32 / 1e6:.1f} MB")
        print(f"FP16: {activation_fp16 / 1e6:.1f} MB")

# Quick demo
comparison = PrecisionComparison()
comparison.memory_usage_comparison()
```

The numbers are compelling. FP16 gives us roughly 2x memory savings for both parameters and activations. But here's where it gets interesting: the speedup isn't just from using less memory. Modern GPUs have specialized tensor cores that can perform FP16 operations significantly faster than FP32. The H100's tensor cores can deliver up to 989 TOPS for FP16, compared to 67 TFLOPS for FP32. That's not a typo – we're talking about an order of magnitude difference in peak theoretical performance.

## The Numerical Precision Hierarchy: FP32, FP16, BF16, and Beyond

Not all reduced precision formats are created equal. The choice between FP16, BF16, and other formats involves understanding the fundamental tradeoffs between numerical range, precision, and hardware support.

```python
import numpy as np
import torch

class PrecisionAnalyzer:
    """Analyze different floating point formats"""
    
    def __init__(self):
        self.formats = {
            'FP32': {'sign': 1, 'exponent': 8, 'mantissa': 23},
            'FP16': {'sign': 1, 'exponent': 5, 'mantissa': 10},
            'BF16': {'sign': 1, 'exponent': 8, 'mantissa': 7},
        }
        
    def analyze_range_and_precision(self):
        """Compare numerical properties of different formats"""
        
        print("Floating Point Format Analysis:")
        print("-" * 50)
        
        for format_name, bits in self.formats.items():
            # Calculate range
            max_exponent = 2**(bits['exponent'] - 1) - 1
            min_exponent = -max_exponent + 1
            
            largest_normal = 2**max_exponent * (2 - 2**(-bits['mantissa']))
            smallest_normal = 2**min_exponent
            
            # Calculate precision (machine epsilon)
            machine_eps = 2**(-bits['mantissa'])
            
            print(f"\n{format_name}:")
            print(f"  Largest normal: {largest_normal:.2e}")
            print(f"  Smallest normal: {smallest_normal:.2e}")
            print(f"  Machine epsilon: {machine_eps:.2e}")
            print(f"  Dynamic range: {np.log10(largest_normal/smallest_normal):.1f} orders of magnitude")
            
    def gradient_underflow_demo(self):
        """Demonstrate gradient underflow in different precisions"""
        
        # Simulate tiny gradients that might underflow
        gradients = torch.tensor([1e-4, 1e-5, 1e-6, 1e-7, 1e-8], dtype=torch.float32)
        
        print("\nGradient Underflow Analysis:")
        print("-" * 30)
        
        for dtype, name in [(torch.float32, 'FP32'), (torch.float16, 'FP16'), (torch.bfloat16, 'BF16')]:
            converted = gradients.to(dtype)
            underflowed = (converted == 0).sum().item()
            
            print(f"{name}: {underflowed}/5 gradients underflowed to zero")
            print(f"  Values: {converted.tolist()}")

analyzer = PrecisionAnalyzer()
analyzer.analyze_range_and_precision()
analyzer.gradient_underflow_demo()
```

The key insight here is that FP16 and BF16 make different tradeoffs. FP16 has higher precision (10 mantissa bits vs 7) but smaller range (5 exponent bits vs 8). BF16 has the same range as FP32 but lower precision. For neural network training, range often matters more than precision – we'd rather represent a tiny gradient approximately than lose it entirely to underflow.

This is why Google chose BF16 for TPUs and why it's becoming increasingly popular for training large models. The extended range means fewer gradients vanish into numerical zero, which can be critical for training stability.

## Automatic Mixed Precision: The PyTorch AutoCast Magic

PyTorch's Automatic Mixed Precision (AMP) is basically a sophisticated type system for floating point operations. It automatically chooses the appropriate precision for each operation based on numerical stability requirements. Matrix multiplications get FP16 for speed, while loss computations stay in FP32 for accuracy.

```python
import torch
import torch.nn as nn
from torch.cuda.amp import autocast, GradScaler
import time

class MixedPrecisionTrainer:
    """Production-ready mixed precision training setup"""
    
    def __init__(self, model: nn.Module):
        self.model = model.cuda()
        self.scaler = GradScaler()
        self.optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
        
        # Track performance metrics
        self.step_times = []
        self.memory_usage = []
        
    def training_step_fp32(self, batch, targets):
        """Traditional FP32 training step"""
        
        start_time = time.perf_counter()
        torch.cuda.reset_peak_memory_stats()
        
        # Standard forward/backward pass
        self.optimizer.zero_grad()
        output = self.model(batch)
        loss = nn.CrossEntropyLoss()(output, targets)
        loss.backward()
        self.optimizer.step()
        
        step_time = time.perf_counter() - start_time
        peak_memory = torch.cuda.max_memory_allocated() / 1e9  # GB
        
        return loss.item(), step_time, peak_memory
        
    def training_step_mixed_precision(self, batch, targets):
        """Mixed precision training step with AMP"""
        
        start_time = time.perf_counter()
        torch.cuda.reset_peak_memory_stats()
        
        self.optimizer.zero_grad()
        
        # Forward pass with autocast
        with autocast():
            output = self.model(batch)
            loss = nn.CrossEntropyLoss()(output, targets)
            
        # Scaled backward pass
        self.scaler.scale(loss).backward()
        
        # Gradient unscaling and clipping
        self.scaler.unscale_(self.optimizer)
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        
        # Optimizer step with scaling
        self.scaler.step(self.optimizer)
        self.scaler.update()
        
        step_time = time.perf_counter() - start_time
        peak_memory = torch.cuda.max_memory_allocated() / 1e9  # GB
        
        return loss.item(), step_time, peak_memory
    
    def benchmark_comparison(self, num_steps=100):
        """Compare FP32 vs Mixed Precision performance"""
        
        # Generate synthetic data
        batch_size, seq_len, hidden_dim = 32, 512, 1024
        batch = torch.randn(batch_size, seq_len, hidden_dim).cuda()
        targets = torch.randint(0, 1000, (batch_size,)).cuda()
        
        print("Benchmarking FP32 vs Mixed Precision...")
        
        # FP32 benchmark
        fp32_losses, fp32_times, fp32_memory = [], [], []
        for _ in range(num_steps):
            loss, step_time, memory = self.training_step_fp32(batch, targets)
            fp32_losses.append(loss)
            fp32_times.append(step_time)
            fp32_memory.append(memory)
            
        # Mixed precision benchmark
        mp_losses, mp_times, mp_memory = [], [], []
        for _ in range(num_steps):
            loss, step_time, memory = self.training_step_mixed_precision(batch, targets)
            mp_losses.append(loss)
            mp_times.append(step_time)
            mp_memory.append(memory)
            
        # Results
        avg_fp32_time = sum(fp32_times) / len(fp32_times)
        avg_mp_time = sum(mp_times) / len(mp_times)
        avg_fp32_memory = sum(fp32_memory) / len(fp32_memory)
        avg_mp_memory = sum(mp_memory) / len(mp_memory)
        
        print(f"\nPerformance Comparison:")
        print(f"FP32 - Time: {avg_fp32_time*1000:.1f}ms, Memory: {avg_fp32_memory:.2f}GB")
        print(f"Mixed - Time: {avg_mp_time*1000:.1f}ms, Memory: {avg_mp_memory:.2f}GB")
        print(f"Speedup: {avg_fp32_time/avg_mp_time:.2f}x")
        print(f"Memory savings: {(1-avg_mp_memory/avg_fp32_memory)*100:.1f}%")
        
        return {
            'speedup': avg_fp32_time / avg_mp_time,
            'memory_savings': 1 - avg_mp_memory / avg_fp32_memory,
            'convergence_difference': abs(fp32_losses[-1] - mp_losses[-1])
        }

# Demo with a simple transformer layer
class SimpleTransformerLayer(nn.Module):
    def __init__(self, d_model=1024, nhead=16):
        super().__init__()
        self.self_attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)
        self.linear1 = nn.Linear(d_model, d_model * 4)
        self.linear2 = nn.Linear(d_model * 4, d_model)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.activation = nn.ReLU()
        
    def forward(self, x):
        # Self attention block
        attn_out, _ = self.self_attn(x, x, x)
        x = self.norm1(x + attn_out)
        
        # Feed forward block  
        ff_out = self.linear2(self.activation(self.linear1(x)))
        x = self.norm2(x + ff_out)
        
        return x

model = SimpleTransformerLayer()
trainer = MixedPrecisionTrainer(model)
results = trainer.benchmark_comparison()
```

What's happening under the hood is quite sophisticated. The `autocast` context manager maintains a whitelist of operations that are safe to run in FP16 (like matrix multiplications) and a blacklist of operations that should stay in FP32 (like loss computations and batch normalization). This automatic type promotion ensures numerical stability while maximizing the performance benefits of reduced precision.

## Gradient Scaling: Preventing the Vanishing Gradient Apocalypse

Here's where mixed precision training gets philosophically interesting. The fundamental problem is that gradients in deep networks are often very small – much smaller than the smallest representable FP16 number. In FP32, a gradient of 1e-7 is perfectly fine. In FP16, it underflows to zero, and your model stops learning.

The solution is gradient scaling: we multiply the loss by a large constant before computing gradients, then scale the gradients back down before the optimizer step. It's like temporarily amplifying a quiet signal to prevent it from getting lost in the noise.

```python
class GradientScalingAnalysis:
    """Deep dive into gradient scaling strategies"""
    
    def __init__(self):
        self.scale_values = [2**i for i in range(1, 16)]  # Powers of 2 from 2 to 32768
        
    def analyze_gradient_distribution(self, model: nn.Module, data_loader, num_batches=10):
        """Analyze the distribution of gradient magnitudes"""
        
        gradient_norms = []
        
        # Collect gradient statistics
        for batch_idx, (batch, targets) in enumerate(data_loader):
            if batch_idx >= num_batches:
                break
                
            # Forward pass
            output = model(batch.cuda())
            loss = nn.CrossEntropyLoss()(output, targets.cuda())
            
            # Backward pass
            model.zero_grad()
            loss.backward()
            
            # Collect gradient norms
            for name, param in model.named_parameters():
                if param.grad is not None:
                    grad_norm = param.grad.norm().item()
                    gradient_norms.append((name, grad_norm))
                    
        # Analysis
        all_norms = [norm for _, norm in gradient_norms]
        min_norm = min(all_norms)
        max_norm = max(all_norms)
        median_norm = sorted(all_norms)[len(all_norms)//2]
        
        print(f"Gradient Analysis:")
        print(f"  Min gradient norm: {min_norm:.2e}")
        print(f"  Median gradient norm: {median_norm:.2e}")  
        print(f"  Max gradient norm: {max_norm:.2e}")
        
        # Check FP16 representability
        fp16_min = 2**(-14)  # Smallest FP16 subnormal
        underflow_count = sum(1 for norm in all_norms if norm < fp16_min)
        print(f"  Gradients that would underflow in FP16: {underflow_count}/{len(all_norms)} ({underflow_count/len(all_norms)*100:.1f}%)")
        
        return min_norm, max_norm, median_norm
        
    def optimal_scale_search(self, model: nn.Module, data_loader):
        """Find optimal gradient scaling factor"""
        
        min_norm, max_norm, median_norm = self.analyze_gradient_distribution(model, data_loader)
        
        # We want to scale gradients so the median is well within FP16 range
        # But not so much that large gradients overflow
        fp16_max = 65504  # Max FP16 value
        
        # Conservative approach: scale so median gradient is around 1e-3
        target_median = 1e-3
        suggested_scale = target_median / median_norm
        
        # But make sure we don't overflow the largest gradients
        max_safe_scale = fp16_max / (max_norm * 2)  # Factor of 2 for safety
        
        optimal_scale = min(suggested_scale, max_safe_scale)
        
        # Round to nearest power of 2 for efficiency
        optimal_scale = 2 ** round(np.log2(optimal_scale))
        
        print(f"\nOptimal Scale Analysis:")
        print(f"  Target median gradient: {target_median:.2e}")
        print(f"  Suggested scale: {suggested_scale:.0f}")
        print(f"  Max safe scale: {max_safe_scale:.0f}")
        print(f"  Optimal scale (power of 2): {optimal_scale:.0f}")
        
        return optimal_scale

class AdaptiveGradScaler:
    """More sophisticated gradient scaler that adapts during training"""
    
    def __init__(self, init_scale=65536.0, growth_factor=2.0, backoff_factor=0.5, growth_interval=2000):
        self.scale = init_scale
        self.growth_factor = growth_factor
        self.backoff_factor = backoff_factor
        self.growth_interval = growth_interval
        
        # Tracking variables
        self.growth_tracker = 0
        self.inf_count = 0
        self.total_steps = 0
        
        print(f"Initialized AdaptiveGradScaler with scale={init_scale}")
        
    def scale_gradients(self, loss):
        """Scale loss for backward pass"""
        return loss * self.scale
        
    def unscale_gradients(self, optimizer):
        """Unscale gradients and check for infs/nans"""
        
        has_inf = False
        
        for group in optimizer.param_groups:
            for param in group['params']:
                if param.grad is not None:
                    param.grad.div_(self.scale)
                    
                    # Check for inf/nan
                    if torch.isinf(param.grad).any() or torch.isnan(param.grad).any():
                        has_inf = True
                        break
            if has_inf:
                break
                
        return has_inf
        
    def update_scale(self, has_inf):
        """Update scaling factor based on whether we found infs"""
        
        self.total_steps += 1
        
        if has_inf:
            # Overflow detected, reduce scale
            self.scale *= self.backoff_factor
            self.growth_tracker = 0
            self.inf_count += 1
            print(f"Step {self.total_steps}: Overflow detected, reducing scale to {self.scale:.0f}")
        else:
            # No overflow, consider increasing scale
            self.growth_tracker += 1
            
            if self.growth_tracker >= self.growth_interval:
                self.scale *= self.growth_factor
                self.growth_tracker = 0
                print(f"Step {self.total_steps}: Increasing scale to {self.scale:.0f}")
                
    def get_stats(self):
        """Get training statistics"""
        overflow_rate = self.inf_count / max(self.total_steps, 1)
        return {
            'current_scale': self.scale,
            'total_steps': self.total_steps,
            'overflow_count': self.inf_count,
            'overflow_rate': overflow_rate
        }

# Example usage in training loop
def training_loop_with_adaptive_scaling():
    """Example of training with adaptive gradient scaling"""
    
    model = SimpleTransformerLayer().cuda()
    optimizer = torch.optim.Adam(model.parameters())
    scaler = AdaptiveGradScaler(init_scale=32768.0)
    
    # Synthetic training data
    batch_size, seq_len, hidden_dim = 16, 256, 1024
    
    for step in range(1000):
        # Generate batch
        batch = torch.randn(batch_size, seq_len, hidden_dim).cuda()
        targets = torch.randint(0, 1000, (batch_size,)).cuda()
        
        optimizer.zero_grad()
        
        # Forward pass with autocast
        with autocast():
            output = model(batch)
            loss = nn.CrossEntropyLoss()(output, targets)
            
        # Scale loss and backward pass
        scaled_loss = scaler.scale_gradients(loss)
        scaled_loss.backward()
        
        # Unscale and check for overflows
        has_inf = scaler.unscale_gradients(optimizer)
        
        if not has_inf:
            # Safe to take optimizer step
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
        # Update scaling factor
        scaler.update_scale(has_inf)
        
        if step % 100 == 0:
            stats = scaler.get_stats()
            print(f"Step {step}: Loss={loss.item():.4f}, Scale={stats['current_scale']:.0f}, Overflow Rate={stats['overflow_rate']:.3f}")

# Run the demo
# training_loop_with_adaptive_scaling()
```

The beauty of adaptive gradient scaling is that it's essentially a feedback control system for numerical precision. When gradients overflow (indicating our scale is too high), we back off. When we go many steps without overflow (indicating we could safely scale higher), we increase the scale to preserve more gradient information.

## Memory Layout Optimization: The Devil in the CUDA Details

Mixed precision isn't just about choosing FP16 vs FP32 – it's about optimizing memory layouts to maximize GPU throughput. Modern GPUs have complex memory hierarchies, and the way you store and access mixed-precision data can dramatically impact performance.

```python
class MemoryLayoutOptimizer:
    """Optimize memory layouts for mixed precision training"""
    
    def __init__(self):
        self.device = torch.cuda.current_device()
        
    def benchmark_tensor_layouts(self, shapes, dtypes):
        """Benchmark different tensor storage patterns"""
        
        results = {}
        
        for shape in shapes:
            for dtype in dtypes:
                # Contiguous layout
                tensor_c = torch.randn(shape, dtype=dtype, device='cuda')
                
                # Strided layout (simulating transpose or permute)
                tensor_s = tensor_c.transpose(-1, -2)
                
                # Benchmark matrix multiplication
                other = torch.randn(shape[-1], 512, dtype=dtype, device='cuda')
                
                # Warmup
                for _ in range(10):
                    _ = torch.matmul(tensor_c, other)
                    _ = torch.matmul(tensor_s, other)
                torch.cuda.synchronize()
                
                # Benchmark contiguous
                start_time = time.perf_counter()
                for _ in range(100):
                    result_c = torch.matmul(tensor_c, other)
                torch.cuda.synchronize()
                time_contiguous = time.perf_counter() - start_time
                
                # Benchmark strided
                start_time = time.perf_counter()
                for _ in range(100):
                    result_s = torch.matmul(tensor_s, other)
                torch.cuda.synchronize()
                time_strided = time.perf_counter() - start_time
                
                key = f"{shape}_{dtype}"
                results[key] = {
                    'contiguous_time': time_contiguous,
                    'strided_time': time_strided,
                    'slowdown': time_strided / time_contiguous
                }
                
        return results
    
    def analyze_tensor_core_utilization(self):
        """Analyze tensor core utilization for different shapes/types"""
        
        # Tensor cores work best with specific shape alignments
        shapes = [
            (16, 64, 64),    # Good alignment
            (17, 65, 65),    # Poor alignment
            (32, 128, 128),  # Excellent alignment
            (31, 127, 127),  # Poor alignment
        ]
        
        dtypes = [torch.float16, torch.bfloat16]
        
        print("Tensor Core Utilization Analysis:")
        print("-" * 50)
        
        for shape in shapes:
            for dtype in dtypes:
                A = torch.randn(shape[0], shape[1], dtype=dtype, device='cuda')
                B = torch.randn(shape[1], shape[2], dtype=dtype, device='cuda')
                
                # Warmup
                for _ in range(10):
                    _ = torch.matmul(A, B)
                torch.cuda.synchronize()
                
                # Benchmark
                start_time = time.perf_counter()
                for _ in range(1000):
                    result = torch.matmul(A, B)
                torch.cuda.synchronize()
                elapsed = time.perf_counter() - start_time
                
                # Calculate theoretical FLOPS
                flops = 2 * shape[0] * shape[1] * shape[2] * 1000  # 1000 iterations
                achieved_tflops = flops / elapsed / 1e12
                
                dtype_name = str(dtype).split('.')[-1]
                print(f"Shape {shape}, {dtype_name}: {achieved_tflops:.1f} TFLOPS")
                
        print("\nNote: Shapes aligned to multiples of 8/16 typically achieve higher TFLOPS")
        print("due to better tensor core utilization.")

class MixedPrecisionMemoryManager:
    """Advanced memory management for mixed precision training"""
    
    def __init__(self, model: nn.Module):
        self.model = model
        self.fp32_parameters = []
        self.fp16_parameters = []
        self.parameter_groups = {}
        
        self.analyze_parameter_precision_requirements()
        
    def analyze_parameter_precision_requirements(self):
        """Categorize parameters by precision requirements"""
        
        for name, param in self.model.named_parameters():
            if any(keyword in name.lower() for keyword in ['norm', 'bias']):
                # Keep normalization layers and biases in FP32
                self.fp32_parameters.append((name, param))
            else:
                # Main weights can use FP16
                self.fp16_parameters.append((name, param))
                
        print(f"Parameter Precision Analysis:")
        print(f"  FP32 parameters: {len(self.fp32_parameters)}")
        print(f"  FP16 parameters: {len(self.fp16_parameters)}")
        
    def create_parameter_groups(self):
        """Create optimizer parameter groups with different precisions"""
        
        fp32_group = {
            'params': [param for _, param in self.fp32_parameters],
            'lr': 1e-4,
            'precision': 'fp32'
        }
        
        fp16_group = {
            'params': [param for _, param in self.fp16_parameters],
            'lr': 1e-4,
            'precision': 'fp16'
        }
        
        return [fp32_group, fp16_group]
    
    def convert_model_precision(self):
        """Convert model to mixed precision"""
        
        for name, param in self.fp16_parameters:
            # Convert to FP16 in-place
            param.data = param.data.half()
            
        print("Model converted to mixed precision")
        
    def get_memory_stats(self):
        """Calculate memory usage statistics"""
        
        fp32_memory = sum(param.numel() * 4 for _, param in self.fp32_parameters)
        fp16_memory = sum(param.numel() * 2 for _, param in self.fp16_parameters)
        
        total_memory = fp32_memory + fp16_memory
        pure_fp32_memory = sum(param.numel() * 4 for _, param in self.model.named_parameters())
        
        savings = (pure_fp32_memory - total_memory) / pure_fp32_memory
        
        return {
            'fp32_
